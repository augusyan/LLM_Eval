{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3c04f7-a871-467e-8dfb-e60c4e8e359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/ytw_llm/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/ytw_llm/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig, AutoModel, BitsAndBytesConfig\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c554edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  #（保证程序cuda序号与实际cuda序号对应）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"  #（代表仅使用第0，1号GPU）\n",
    "torch.cuda.set_device(1) # 只有这句可以其效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ce98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.24s/it]\n"
     ]
    }
   ],
   "source": [
    "## 模型加载\n",
    "#使用QLoRA引入的 NF4量化数据类型以节约显存\n",
    "model_name_or_path ='/work/ytw/LLM/Baichuan2-13B-Chat' #远程 'baichuan-inc/Baichuan-13B-Chat'\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "   model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                quantization_config=bnb_config,\n",
    "                trust_remote_code=True) \n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a393523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "世界上第二高的山峰是乔戈里峰（K2），海拔8,611米（28,251英尺）。它位于巴基斯坦和中国边境的喀喇昆仑山脉。\n"
     ]
    }
   ],
   "source": [
    "# 测试用例\n",
    "from IPython.display import clear_output \n",
    "messages = []\n",
    "messages.append({\"role\": \"user\",\n",
    "                 \"content\": \"世界上第二高的山峰是哪座?\"})\n",
    "response = model.chat(tokenizer,messages=messages,stream=True)\n",
    "for res in response:\n",
    "    print(res)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8324b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '''命名实体识别：抽取文本中的 人名，地点，组织 这三类命名实体，并按照json格式返回结果。\n",
    "\n",
    "下面是一些范例：\n",
    "\n",
    "小明对小红说:\"你听说过安利吗？\" -> {\"人名\": [\"小明\",\"小红\"], \"组织\": [\"安利\"]}\n",
    "现在，每年有几十万中国人到美国访问，几千名中国留学生到美国就学。 -> {\"地点\": [\"中国\", \"美国\"]}\n",
    "中国是联合国安理会常任理事国之一。 -> {\"地点\": [\"中国\"], \"组织\": [\"联合国\"]}\n",
    "\n",
    "请对下述文本进行实体抽取，返回json格式。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ded3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构造简单的few-shot prompt\n",
    "def get_prompt(text):\n",
    "    return prefix+text+' -> '\n",
    "\n",
    "def get_message(prompt,response):\n",
    "    return [{\"role\": \"user\", \"content\": f'{prompt} -> '},\n",
    "            {\"role\": \"assistant\", \"content\": response}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58dd1279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"人名\": [], \"地点\": [\"摩洛哥\"], \"组织\": []}\n"
     ]
    }
   ],
   "source": [
    "messages  = [{\"role\": \"user\", \"content\": get_prompt(\"一些摩洛哥球迷已按捺不住，在看台上欢呼雀跃\")}]\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac66a234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': '命名实体识别：抽取文本中的 人名，地点，组织 这三类命名实体，并按照json格式返回结果。\\n\\n下面是一些范例：\\n\\n小明对小红说:\"你听说过安利吗？\" -> {\"人名\": [\"小明\",\"小红\"], \"组织\": [\"安利\"]}\\n现在，每年有几十万中国人到美国访问，几千名中国留学生到美国就学。 -> {\"地点\": [\"中国\", \"美国\"]}\\n中国是联合国安理会常任理事国之一。 -> {\"地点\": [\"中国\"], \"组织\": [\"联合国\"]}\\n\\n请对下述文本进行实体抽取，返回json格式。\\n\\n一些摩洛哥球迷已按捺不住，在看台上欢呼雀跃 -> '}, {'role': 'assistant', 'content': \"{'地点': ['摩洛哥']}\"}, {'role': 'user', 'content': '这次轮到北京国安队，不知会不会再步后尘？ -> '}, {'role': 'assistant', 'content': \"{'组织': ['北京国安队']}\"}, {'role': 'user', 'content': '革命党人孙中山在澳门成立同盟会分会 -> '}, {'role': 'assistant', 'content': \"{'人名': ['孙中山'], '地名': ['澳门'], '组织': ['同盟会']}\"}, {'role': 'user', 'content': '我曾在安徽芜湖市和上海浦东打工。 -> '}, {'role': 'assistant', 'content': \"{'地点': ['安徽芜湖市', '上海浦东']}\"}]\n"
     ]
    }
   ],
   "source": [
    "messages = messages+[{\"role\": \"assistant\", \"content\": \"{'地点': ['摩洛哥']}\"}]\n",
    "messages.extend(get_message(\"这次轮到北京国安队，不知会不会再步后尘？\",\"{'组织': ['北京国安队']}\"))\n",
    "messages.extend(get_message(\"革命党人孙中山在澳门成立同盟会分会\",\"{'人名': ['孙中山'], '地名': ['澳门'], '组织': ['同盟会']}\"))\n",
    "messages.extend(get_message(\"我曾在安徽芜湖市和上海浦东打工。\",\"{'地点': ['安徽芜湖市', '上海浦东']}\"))\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a58851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,temperature=0.01):\n",
    "    model.generation_config.temperature=temperature\n",
    "    response = model.chat(tokenizer, \n",
    "                          messages = messages+[{'role':'user','content':f'{text} -> '}])\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "483bb901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'人名': ['李白', '杜甫']}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('杜甫是李白的粉丝。') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fc011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [09:51<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unhashable type: 'dict'\n",
      "precision = 0.49033816425120774\n",
      "recall = 0.5858585858585859\n",
      "f1_score = 0.5338593030900723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用作者给出的简单ner数据集进行测试\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_pickle('dfner_13k.pkl')\n",
    "dfdata,dftest = train_test_split(df,test_size=300,random_state=42)\n",
    "dftrain,dfval = train_test_split(dfdata,test_size=200,random_state=42)\n",
    "\n",
    "preds = ['' for x in dftest['target']]\n",
    "for i in tqdm(range(len(preds))):\n",
    "    preds[i] = predict(dftest['text'].iloc[i])\n",
    "    \n",
    "\n",
    "def toset(s):\n",
    "    try:\n",
    "        dic = eval(str(s))\n",
    "        res = []\n",
    "        for k,v in dic.items():\n",
    "            for x in v:\n",
    "                if x:\n",
    "                    res.append((k,x))\n",
    "        return set(res)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return set()\n",
    "\n",
    "dftest['pred'] = [toset(x) for x in preds]\n",
    "dftest['gt'] = [toset(x) for x in dftest['target']]\n",
    "dftest['tp_cnt'] = [len(pred&gt) for pred,gt in zip(dftest['pred'],dftest['gt'])]\n",
    "dftest['pred_cnt'] = [len(x) for x in dftest['pred']]\n",
    "dftest['gt_cnt'] = [len(x) for x in dftest['gt']]\n",
    "\n",
    "precision = sum(dftest['tp_cnt'])/sum(dftest['pred_cnt'])\n",
    "print('precision = '+str(precision))\n",
    "\n",
    "recall = sum(dftest['tp_cnt'])/sum(dftest['gt_cnt'])\n",
    "print('recall = '+str(recall))\n",
    "\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print('f1_score = '+str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f0265",
   "metadata": {},
   "source": [
    "我们仿照百川模型的 model._build_chat_input 方法来进行token编码，同时把需要学习的内容添加label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb8ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "#将messages编码成 token, 同时返回labels\n",
    "#注意baichuan-13b通过插入tokenizer.user_token_id和tokenizer.assistant_token_id 来区分用户和机器人会话内容\n",
    "\n",
    "# reference@ model._build_chat_input?\n",
    "def build_chat_input(messages, model=model,\n",
    "                     tokenizer=tokenizer, \n",
    "                     max_new_tokens: int=0):\n",
    "    max_new_tokens = max_new_tokens or model.generation_config.max_new_tokens\n",
    "    max_input_tokens = model.config.model_max_length - max_new_tokens\n",
    "    max_input_tokens = max(model.config.model_max_length // 2, max_input_tokens)\n",
    "    \n",
    "    total_input, round_input, total_label, round_label = [], [], [], []\n",
    "    \n",
    "    for i, message in enumerate(messages[::-1]):\n",
    "        content_tokens = tokenizer.encode(message['content'])\n",
    "        if message['role'] == 'user':\n",
    "            round_input = [model.generation_config.user_token_id] + content_tokens + round_input\n",
    "            round_label = [-100]+[-100 for _ in content_tokens]+ round_label\n",
    "            \n",
    "            if total_input and len(total_input) + len(round_input) > max_input_tokens:\n",
    "                break\n",
    "            else:\n",
    "                total_input = round_input + total_input\n",
    "                total_label = round_label + total_label\n",
    "                if len(total_input) >= max_input_tokens:\n",
    "                    break\n",
    "                else:\n",
    "                    round_input = []\n",
    "                    round_label = []\n",
    "                    \n",
    "        elif message['role'] == 'assistant':\n",
    "            round_input = [\n",
    "                model.generation_config.assistant_token_id\n",
    "            ] + content_tokens + [\n",
    "                model.generation_config.eos_token_id\n",
    "            ] + round_input\n",
    "            \n",
    "            if i==0: #仅对最后一轮的target进行学习\n",
    "                round_label = [\n",
    "                    -100\n",
    "                ] + content_tokens + [\n",
    "                    model.generation_config.eos_token_id\n",
    "                ]+ round_label\n",
    "            else:\n",
    "                round_label = [\n",
    "                    -100\n",
    "                ] + [-100 for _ in content_tokens] + [\n",
    "                    -100\n",
    "                ]+ round_label\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(f\"message role not supported yet: {message['role']}\")\n",
    "            \n",
    "    total_input = total_input[-max_input_tokens:]  # truncate left\n",
    "    total_label = total_label[-max_input_tokens:]\n",
    "    \n",
    "    total_input.append(model.generation_config.assistant_token_id)\n",
    "    total_label.append(-100)\n",
    "    \n",
    "    return total_input,total_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做数据集\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "from copy import deepcopy\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,df,\n",
    "                 messages\n",
    "                ):\n",
    "        self.df = df \n",
    "        self.messages = messages\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def get_samples(self,index):\n",
    "        samples = []\n",
    "        d = dict(self.df.iloc[index])\n",
    "        samples.append(d)\n",
    "        return samples\n",
    "    \n",
    "    def get_messages(self,index):\n",
    "        samples = self.get_samples(index)\n",
    "        messages = deepcopy(self.messages)\n",
    "        for i,d in enumerate(samples):\n",
    "\n",
    "            messages.append({'role':'user','content':d['text']+' -> '})\n",
    "            messages.append({'role':'assistant','content':str(d['target'])})\n",
    "        return messages\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        messages = self.get_messages(index)\n",
    "        input_ids, labels = build_chat_input(messages)\n",
    "        return {'input_ids':input_ids,'labels':labels}\n",
    "\n",
    "    def show_sample(self,index):\n",
    "        samples = self.get_samples(index)\n",
    "        print(samples)\n",
    "    \n",
    "    \n",
    "\n",
    "ds_train = MyDataset(dftrain,messages)\n",
    "ds_val = MyDataset(dfval,messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动释放显存\n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
